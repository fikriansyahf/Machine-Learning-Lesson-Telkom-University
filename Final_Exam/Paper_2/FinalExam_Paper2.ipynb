{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalExam_Paper2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Process Data Train"
      ],
      "metadata": {
        "id": "i308kPqgjiP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO-VMPz7xhC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd54d336-f741-4d5d-dbb5-a54de32fba32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: dataset/train-processed-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 code/preprocess.py dataset/train-processed.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process Data Test"
      ],
      "metadata": {
        "id": "Ar9JrEv-jmU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/preprocess.py dataset/test-processed.csv"
      ],
      "metadata": {
        "id": "ct8lWDlazXkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee073e79-3355-4da1-d247-c327de2591ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"code/preprocess.py\", line 107, in <module>\n",
            "    preprocess_csv(csv_file_name, processed_file_name, test_file=False)\n",
            "  File \"code/preprocess.py\", line 81, in preprocess_csv\n",
            "    positive = int(line[:line.find(',')])\n",
            "ValueError: invalid literal for int() with base 10: 'is so sad for my apl friend'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process New Data Train "
      ],
      "metadata": {
        "id": "ZvJHtO7-jwXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py dataset/train-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05eOJaQuz4Ot",
        "outputId": "7614aaa4-3853-4403-82d8-7985494908ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/train-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/train-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 0, Avg: 0.0000, Max: 0\n",
            "URLs => Total: 0, Avg: 0.0000, Max: 0\n",
            "Emojis => Total: 0, Positive: 0, Negative: 0, Avg: 0.0000, Max: 0\n",
            "Words => Total: 1283269, Unique: 50361, Avg: 12.8327, Max: 41, Min: 0\n",
            "Bigrams => Total: 1183437, Unique: 392543, Avg: 11.8344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process New Data Test"
      ],
      "metadata": {
        "id": "TaxzXjelj3vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py dataset/test-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8guCYQd70Gil",
        "outputId": "8ddf9ddc-12db-4e82-f12c-b299e4cab6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/test-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/test-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 0, Positive: 0, Negative: 0\n",
            "Traceback (most recent call last):\n",
            "  File \"code/stats.py\", line 106, in <module>\n",
            "    print 'User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions)\n",
            "ZeroDivisionError: float division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Baseline"
      ],
      "metadata": {
        "id": "f0jm8C2sj5u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh6znQlX0iK8",
        "outputId": "b3e2b404-2a7f-4366-85fe-48e40b2cbca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classificatiion using NaiveBayes"
      ],
      "metadata": {
        "id": "1EnIkLKmjUC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls9uwkvA0mce",
        "outputId": "7c5e428d-29e7-41b1-a22d-84f56a7fabce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7725/10000 = 77.2500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic"
      ],
      "metadata": {
        "id": "95WAfMf9jNy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/logistic.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tsbj_Vk1HEP",
        "outputId": "a48fe056-8c84-40b7-9295-c84880f80ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-22 03:32:30.316943: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Iteration 12/180, loss:0.6857, acc:0.58802022-01-22 03:32:33.924502: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 75/180, loss:0.6555, acc:0.66202022-01-22 03:32:41.402085: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 131/180, loss:0.6069, acc:0.74202022-01-22 03:32:48.063986: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 134/180, loss:0.6180, acc:0.71002022-01-22 03:32:48.426555: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 157/180, loss:0.6002, acc:0.74402022-01-22 03:32:51.175084: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5962, acc:0.72002022-01-22 03:32:55.582167: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 1, val_acc:0.7357\n",
            "Accuracy improved from 0.0000 to 0.7357, saving model\n",
            "Iteration 30/180, loss:0.5924, acc:0.71602022-01-22 03:32:59.912171: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 69/180, loss:0.5834, acc:0.72402022-01-22 03:33:04.534843: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5299, acc:0.77002022-01-22 03:33:18.482109: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 2, val_acc:0.7516\n",
            "Accuracy improved from 0.7357 to 0.7516, saving model\n",
            "Iteration 5/180, loss:0.5347, acc:0.78402022-01-22 03:33:20.869595: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 11/180, loss:0.5826, acc:0.73802022-01-22 03:33:21.580283: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 80/180, loss:0.5627, acc:0.70802022-01-22 03:33:29.868004: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 83/180, loss:0.5237, acc:0.78202022-01-22 03:33:30.213528: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 100/180, loss:0.5372, acc:0.75802022-01-22 03:33:32.257080: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 158/180, loss:0.5156, acc:0.77202022-01-22 03:33:39.212456: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5255, acc:0.76402022-01-22 03:33:41.825093: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 03:33:43.403683: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 3, val_acc:0.7599\n",
            "Accuracy improved from 0.7516 to 0.7599, saving model\n",
            "Iteration 21/180, loss:0.5172, acc:0.78202022-01-22 03:33:46.640806: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 75/180, loss:0.4899, acc:0.78402022-01-22 03:33:53.051001: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 102/180, loss:0.5003, acc:0.78602022-01-22 03:33:56.203044: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5019, acc:0.7700\n",
            "Epoch: 4, val_acc:0.7660\n",
            "Accuracy improved from 0.7599 to 0.7660, saving model\n",
            "Iteration 95/180, loss:0.4780, acc:0.81002022-01-22 03:34:18.727685: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 97/180, loss:0.4825, acc:0.79602022-01-22 03:34:18.954745: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 156/180, loss:0.4928, acc:0.79202022-01-22 03:34:25.869481: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4847, acc:0.77802022-01-22 03:34:29.443032: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 5, val_acc:0.7714\n",
            "Accuracy improved from 0.7660 to 0.7714, saving model\n",
            "Iteration 29/180, loss:0.4663, acc:0.79402022-01-22 03:34:34.384838: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 162/180, loss:0.4747, acc:0.77002022-01-22 03:34:49.732173: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4836, acc:0.79402022-01-22 03:34:53.019792: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 6, val_acc:0.7752\n",
            "Accuracy improved from 0.7714 to 0.7752, saving model\n",
            "Iteration 122/180, loss:0.4765, acc:0.77002022-01-22 03:35:07.978205: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 155/180, loss:0.4600, acc:0.79802022-01-22 03:35:11.730540: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4484, acc:0.81402022-01-22 03:35:15.007093: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 03:35:15.751814: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 03:35:16.286717: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 7, val_acc:0.7768\n",
            "Accuracy improved from 0.7752 to 0.7768, saving model\n",
            "Iteration 43/180, loss:0.4347, acc:0.83002022-01-22 03:35:21.670944: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 63/180, loss:0.4431, acc:0.82202022-01-22 03:35:23.938266: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 107/180, loss:0.4508, acc:0.81402022-01-22 03:35:29.012222: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4531, acc:0.7900\n",
            "Epoch: 8, val_acc:0.7788\n",
            "Accuracy improved from 0.7768 to 0.7788, saving model\n",
            "Iteration 180/180, loss:0.4669, acc:0.7840\n",
            "Epoch: 9, val_acc:0.7800\n",
            "Accuracy improved from 0.7788 to 0.7800, saving model\n",
            "Iteration 38/180, loss:0.4310, acc:0.80402022-01-22 03:36:06.678618: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 97/180, loss:0.4207, acc:0.83002022-01-22 03:36:13.371807: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 136/180, loss:0.4232, acc:0.81002022-01-22 03:36:17.805771: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4381, acc:0.8160\n",
            "Epoch: 10, val_acc:0.7804\n",
            "Accuracy improved from 0.7800 to 0.7804, saving model\n",
            "Iteration 21/180, loss:0.4266, acc:0.83002022-01-22 03:36:27.480130: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 36/180, loss:0.3989, acc:0.83802022-01-22 03:36:29.179525: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4428, acc:0.81202022-01-22 03:36:46.764251: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 03:36:46.991429: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 03:36:47.322085: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 11, val_acc:0.7813\n",
            "Accuracy improved from 0.7804 to 0.7813, saving model\n",
            "Iteration 50/180, loss:0.3832, acc:0.86402022-01-22 03:36:53.582495: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 176/180, loss:0.4450, acc:0.80802022-01-22 03:37:07.843058: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4337, acc:0.8240\n",
            "Epoch: 12, val_acc:0.7814\n",
            "Accuracy improved from 0.7813 to 0.7814, saving model\n",
            "Iteration 44/180, loss:0.3957, acc:0.82802022-01-22 03:37:15.615789: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 152/180, loss:0.4237, acc:0.81602022-01-22 03:37:27.886649: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4242, acc:0.81402022-01-22 03:37:31.553042: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 13, val_acc:0.7812\n",
            "Iteration 82/180, loss:0.3985, acc:0.83002022-01-22 03:37:42.549008: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 133/180, loss:0.4216, acc:0.81802022-01-22 03:37:48.324085: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 147/180, loss:0.4024, acc:0.83002022-01-22 03:37:49.904649: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4427, acc:0.8100\n",
            "Epoch: 14, val_acc:0.7819\n",
            "Accuracy improved from 0.7814 to 0.7819, saving model\n",
            "Iteration 79/180, loss:0.4090, acc:0.81602022-01-22 03:38:04.737092: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 120/180, loss:0.4174, acc:0.82002022-01-22 03:38:09.372149: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 158/180, loss:0.4084, acc:0.84402022-01-22 03:38:13.634853: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4010, acc:0.81802022-01-22 03:38:17.805086: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 15, val_acc:0.7828\n",
            "Accuracy improved from 0.7819 to 0.7828, saving model\n",
            "Iteration 2/180, loss:0.3866, acc:0.85002022-01-22 03:38:18.539529: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 62/180, loss:0.4062, acc:0.83602022-01-22 03:38:25.327932: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 128/180, loss:0.3649, acc:0.85402022-01-22 03:38:32.777995: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4156, acc:0.82002022-01-22 03:38:39.792093: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 16, val_acc:0.7838\n",
            "Accuracy improved from 0.7828 to 0.7838, saving model\n",
            "Iteration 34/180, loss:0.3901, acc:0.84002022-01-22 03:38:44.766765: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 91/180, loss:0.4087, acc:0.82202022-01-22 03:38:51.181622: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4037, acc:0.8140\n",
            "Epoch: 17, val_acc:0.7839\n",
            "Accuracy improved from 0.7838 to 0.7839, saving model\n",
            "Iteration 17/180, loss:0.4005, acc:0.81802022-01-22 03:39:05.404090: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 18/180, loss:0.3805, acc:0.86602022-01-22 03:39:05.513237: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 48/180, loss:0.4220, acc:0.82002022-01-22 03:39:08.880092: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 85/180, loss:0.3800, acc:0.84802022-01-22 03:39:13.003914: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3964, acc:0.83002022-01-22 03:39:24.934824: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 18, val_acc:0.7833\n",
            "Iteration 46/180, loss:0.3727, acc:0.85802022-01-22 03:39:31.028429: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 94/180, loss:0.3991, acc:0.81002022-01-22 03:39:36.336695: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 101/180, loss:0.3892, acc:0.83802022-01-22 03:39:37.117827: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 160/180, loss:0.4019, acc:0.82802022-01-22 03:39:43.677089: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 176/180, loss:0.3991, acc:0.83202022-01-22 03:39:45.471861: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3863, acc:0.82602022-01-22 03:39:46.890857: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 03:39:47.863098: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 19, val_acc:0.7836\n",
            "Iteration 180/180, loss:0.3879, acc:0.8440\n",
            "Epoch: 20, val_acc:0.7839\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to logistic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWCDtvLq3nSD",
        "outputId": "cb051cda-e45d-4f91-85ee-ce93bac54f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6719/10000 = 67.1900 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification using Random Forest"
      ],
      "metadata": {
        "id": "ttv2iJpRi9uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DELR2JMR4B4B",
        "outputId": "bff9f707-e658-4d81-df36-37eeeaf1e60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7203/10000 = 72.0300 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification using XGBoost"
      ],
      "metadata": {
        "id": "70W_a4fsinmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = 'dataset/train-processed-processed.csv'\n",
        "TEST_PROCESSED_FILE = 'dataset/test-processed-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 1500\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 100\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            utils.write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(20)\n",
        "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = utils.top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = utils.split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=20)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        utils.write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            utils.write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            utils.write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        utils.save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1j0At7VTG5g",
        "outputId": "4bfc69aa-fab3-498c-8786-99c08c304849"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7171/10000 = 71.7100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM Model"
      ],
      "metadata": {
        "id": "nf9uv4w-ikUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/svm.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l76geIdUYwX",
        "outputId": "df41172a-d85b-412c-cfb8-16167258daeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7893/10000 = 78.9300 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/neuralnet.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmQJQWkCUxID",
        "outputId": "1fff214d-5064-441d-d20b-857a566a2251"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2022-01-22 05:52:22.899318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-01-22 05:52:22.905063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:22.905646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-22 05:52:22.939660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 05:52:23.065973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 05:52:23.119315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-22 05:52:23.166337: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-22 05:52:23.325142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-22 05:52:23.412657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-22 05:52:23.708136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-22 05:52:23.708323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.709050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.709634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-22 05:52:23.710085: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-01-22 05:52:23.714600: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-01-22 05:52:23.714782: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ecd3e9ebc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-22 05:52:23.714809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-01-22 05:52:23.886634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.887436: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ecd3e9ed80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-22 05:52:23.887469: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-01-22 05:52:23.887641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.888206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-22 05:52:23.888270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 05:52:23.888292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 05:52:23.888313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-22 05:52:23.888333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-22 05:52:23.888357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-22 05:52:23.888376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-22 05:52:23.888396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-22 05:52:23.888476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.889034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.889530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-22 05:52:23.889638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 05:52:23.890917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-01-22 05:52:23.890947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2022-01-22 05:52:23.890957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2022-01-22 05:52:23.891160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.891731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 05:52:23.892325: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-01-22 05:52:23.892375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14134 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "2022-01-22 05:52:24.632410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "Iteration 180/180, loss:0.5190, acc:0.7560\n",
            "Epoch: 1, val_acc:0.7627\n",
            "Accuracy improved from 0.0000 to 0.7627, saving model\n",
            "Iteration 180/180, loss:0.4664, acc:0.7760\n",
            "Epoch: 2, val_acc:0.7722\n",
            "Accuracy improved from 0.7627 to 0.7722, saving model\n",
            "Iteration 180/180, loss:0.4148, acc:0.8180\n",
            "Epoch: 3, val_acc:0.7717\n",
            "Iteration 180/180, loss:0.4978, acc:0.7420\n",
            "Epoch: 4, val_acc:0.7682\n",
            "Iteration 180/180, loss:0.4185, acc:0.8260\n",
            "Epoch: 5, val_acc:0.7666\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloada Glove File"
      ],
      "metadata": {
        "id": "4-ttT-VTiZTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVDT0yDYV6uL",
        "outputId": "2db00cb0-3750-431a-b8ef-ae513b0d85af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-22 05:54:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-22 05:54:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-22 05:54:58--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 40s  \n",
            "\n",
            "2022-01-22 05:57:39 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open ZIP File of Glove"
      ],
      "metadata": {
        "id": "vyVlEv4FiVbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC6NTHIhW-6t",
        "outputId": "64ead00d-574e-437b-9d6d-60d516171f52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Model"
      ],
      "metadata": {
        "id": "-Mxco7KQiRhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMbOD2dxXWeN",
        "outputId": "6b77df51-8f26-46c9-b28e-acb944ac4027"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "Looking for GLOVE vectors\n",
            "Processing 400000/0\n",
            "\n",
            "Found 31735 words in GLOVE\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2022-01-22 06:12:54.205746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-01-22 06:12:54.211317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.211874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-22 06:12:54.212263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 06:12:54.213744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 06:12:54.214943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-22 06:12:54.215567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-22 06:12:54.217491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-22 06:12:54.218773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-22 06:12:54.222579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-22 06:12:54.222696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.223280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.223781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-22 06:12:54.224068: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-01-22 06:12:54.227904: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-01-22 06:12:54.228277: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561853e6cbc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-22 06:12:54.228307: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-01-22 06:12:54.402931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.403619: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561853e6cd80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-22 06:12:54.403653: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-01-22 06:12:54.403858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.404432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-22 06:12:54.404514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 06:12:54.404540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 06:12:54.404565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-22 06:12:54.404587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-22 06:12:54.404606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-22 06:12:54.404628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-22 06:12:54.404651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-22 06:12:54.404730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.405319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.405828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-22 06:12:54.405907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 06:12:54.407495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-01-22 06:12:54.407531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2022-01-22 06:12:54.407541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2022-01-22 06:12:54.407681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.408530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:12:54.409312: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-01-22 06:12:54.409455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14134 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 40, 200)           18000200  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 200)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               168448    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 18,176,969\n",
            "Trainable params: 18,176,969\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "2022-01-22 06:12:56.606441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "90000/90000 [==============================] - 29s 325us/step - loss: 0.5356 - accuracy: 0.7286 - val_loss: 0.4955 - val_accuracy: 0.7589\n",
            "Epoch 2/5\n",
            "90000/90000 [==============================] - 29s 320us/step - loss: 0.4592 - accuracy: 0.7865 - val_loss: 0.4645 - val_accuracy: 0.7753\n",
            "Epoch 3/5\n",
            "90000/90000 [==============================] - 29s 322us/step - loss: 0.4099 - accuracy: 0.8160 - val_loss: 0.4779 - val_accuracy: 0.7726\n",
            "Epoch 4/5\n",
            "90000/90000 [==============================] - 29s 318us/step - loss: 0.3641 - accuracy: 0.8375 - val_loss: 0.5033 - val_accuracy: 0.7726\n",
            "Epoch 5/5\n",
            "90000/90000 [==============================] - 29s 318us/step - loss: 0.3252 - accuracy: 0.8576 - val_loss: 0.5150 - val_accuracy: 0.7678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Model with 4 Convolution Layer Neural Network"
      ],
      "metadata": {
        "id": "xyMC4Mv_iCzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/cnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPhdxPyobTJq",
        "outputId": "83785183-52ad-495c-bcaa-bbb5e83f23b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend.\n",
            "Looking for GLOVE seeds\n",
            "Processing 400000/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2022-01-22 06:21:40.115624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-01-22 06:21:40.121502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.122062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-22 06:21:40.122314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 06:21:40.123432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 06:21:40.124442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-22 06:21:40.124780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-22 06:21:40.126005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-22 06:21:40.127364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-22 06:21:40.135056: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-22 06:21:40.135170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.135747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.136264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-22 06:21:40.136536: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-01-22 06:21:40.140413: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2022-01-22 06:21:40.140626: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5561ed514bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-22 06:21:40.140654: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-01-22 06:21:40.314355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.315041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5561ed514d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2022-01-22 06:21:40.315075: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2022-01-22 06:21:40.315276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.315810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2022-01-22 06:21:40.315876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 06:21:40.315913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 06:21:40.315935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2022-01-22 06:21:40.315959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2022-01-22 06:21:40.315980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2022-01-22 06:21:40.315999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2022-01-22 06:21:40.316019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2022-01-22 06:21:40.316110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.316682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.317195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2022-01-22 06:21:40.317265: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2022-01-22 06:21:40.318453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2022-01-22 06:21:40.318482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2022-01-22 06:21:40.318496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2022-01-22 06:21:40.318621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.319202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-01-22 06:21:40.319712: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-01-22 06:21:40.319749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14134 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 90000 samples, validate on 10000 samples\n",
            "Epoch 1/8\n",
            "2022-01-22 06:21:42.345533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2022-01-22 06:21:42.536219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "90000/90000 [==============================] - 22s 247us/step - loss: 0.5330 - accuracy: 0.7243 - val_loss: 0.4674 - val_accuracy: 0.7743\n",
            "Epoch 2/8\n",
            "90000/90000 [==============================] - 18s 201us/step - loss: 0.4483 - accuracy: 0.7885 - val_loss: 0.4543 - val_accuracy: 0.7819\n",
            "Epoch 3/8\n",
            "90000/90000 [==============================] - 18s 201us/step - loss: 0.3955 - accuracy: 0.8205 - val_loss: 0.4675 - val_accuracy: 0.7808\n",
            "Epoch 4/8\n",
            "90000/90000 [==============================] - 18s 202us/step - loss: 0.3510 - accuracy: 0.8443 - val_loss: 0.4772 - val_accuracy: 0.7786\n",
            "Epoch 5/8\n",
            "90000/90000 [==============================] - 18s 202us/step - loss: 0.3066 - accuracy: 0.8664 - val_loss: 0.5213 - val_accuracy: 0.7633\n",
            "Epoch 6/8\n",
            "90000/90000 [==============================] - 18s 203us/step - loss: 0.2719 - accuracy: 0.8832 - val_loss: 0.5677 - val_accuracy: 0.7718\n",
            "Epoch 7/8\n",
            "90000/90000 [==============================] - 18s 203us/step - loss: 0.2436 - accuracy: 0.8956 - val_loss: 0.6277 - val_accuracy: 0.7669\n",
            "Epoch 8/8\n",
            "90000/90000 [==============================] - 18s 204us/step - loss: 0.2198 - accuracy: 0.9064 - val_loss: 0.6231 - val_accuracy: 0.7606\n"
          ]
        }
      ]
    }
  ]
}